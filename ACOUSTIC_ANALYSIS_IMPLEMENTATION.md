# Acoustic Voice Analysis Implementation

## Overview
Full voice analysis system implemented with **personalized, user-friendly insights** that translate technical acoustic metrics into conversational, actionable feedback.

**Key Principle**: Compute technical metrics behind the scenes, present insights in everyday language.

---

## What Was Implemented

### Backend (Python/FastAPI)

#### 1. New Modules (`backend/`)

**audio_utils.py**
- Load audio from base64 â†’ numpy array
- Normalize levels, prevent clipping
- Save temporary WAV files
- Audio duration calculation

**vad.py** (Voice Activity Detection)
- WebRTC VAD for speech/silence segmentation
- Compute timing metrics:
  - Speech ratio, silence ratio
  - Pause count, mean pause duration
  - Long pause detection (>700ms)

**feature_extractor.py**
- **Prosody**: Pitch (F0) extraction using librosa PYIN
  - Mean, std, percentiles (p5, p50, p95)
  - Pitch range, time series for visualization
- **Loudness**: RMS energy analysis
  - Mean, std, dynamic range
  - Energy timeline
- **Voice Quality**: Jitter/shimmer/HNR proxies
  - Uses spectral features (librosa-based fallback)
- **Spectral**: MFCCs, centroid, rolloff, bandwidth

**insights_generator.py** â­ **KEY MODULE**
- Translates technical metrics â†’ conversational insights
- Example translations:
  - `pitch_std: 45Hz` â†’ "Your voice has rich melody that keeps listeners engaged"
  - `jitter: 0.8%` â†’ "Your voice quality is crystal clear with no shakiness"
  - `pause_count: 12` â†’ "You used 12 well-timed pauses that show confidence"
- Generates:
  - Voice personality archetype
  - Personalized headline
  - Key insights (5-7 conversational observations)
  - What went well (3-4 strengths)
  - Growth opportunities (3-4 actionable tips)
  - Tone description

**prompt_builder.py**
- Enhanced GPT-4 prompt with acoustic context
- System persona: "Maya" - warm, expert voice coach
- Avoids technical jargon in responses
- Instructs GPT to provide:
  - Voice personality (e.g., "Dynamic Storyteller")
  - Conversational headline
  - Specific, actionable improvements
  - Personalized quick-win tips

#### 2. Modified `server.py`
- **New analysis pipeline** in `/api/analyze-voice`:
  1. Load audio â†’ VAD â†’ extract features
  2. Transcribe with Whisper
  3. Detect filler words, compute pace
  4. Generate rule-based insights
  5. Enhanced GPT prompt with acoustic metrics
  6. Merge GPT + rule-based â†’ final analysis
  
- **New data structure** saved to MongoDB:
  ```javascript
  analysis: {
    insights: {  // User-facing (conversational)
      voice_personality, headline, key_insights[], 
      what_went_well[], growth_opportunities[], 
      tone_description, scores, personalized_tips[]
    },
    metrics: {  // Simplified for UI badges
      speaking_pace, pause_effectiveness, 
      vocal_variety, energy_level, clarity_rating
    },
    technical: {  // Hidden (for debugging)
      prosody, loudness, quality, spectral, timing
    },
    timelines: {  // For visualizations
      pitch[], loudness[], pauses[]
    },
    // Legacy fields (backward compatible)
    archetype, overall_score, strengths[], improvements[], ...
  }
  ```

#### 3. Dependencies Added
```
parselmouth==0.4.3
librosa==0.10.1
soundfile==0.12.1
webrtcvad==2.0.10
scipy==1.11.4
resampy==0.4.2
faster-whisper==0.10.0
```

---

### Frontend (React Native/Expo)

#### 1. Updated `results.tsx`
- **Backward compatible**: Supports both new insights and legacy analysis
- Fallback logic:
  ```typescript
  const personality = analysis.insights?.voice_personality 
                   || analysis.archetype 
                   || "Balanced Communicator";
  ```
- Null-safe handling for all fields

#### 2. New `PersonalizedResults.tsx`
- Modern, conversational UI design
- Sections:
  - ðŸŽ­ **Voice Personality Hero**: Archetype + score + headline
  - ðŸ“Š **Metrics Quick View**: Pace, energy, clarity badges
  - ðŸ’¡ **Key Insights**: 5-7 conversational observations
  - âœ¨ **What You Did Great**: Strength cards
  - ðŸš€ **How to Level Up**: Numbered growth opportunity cards
  - ðŸŽ¯ **Quick Wins**: Actionable tip cards
  - ðŸŽ“ **Training Questions**: Unchanged
  - **Upgrade CTA**: For premium

---

## Translation Examples

| Technical Metric | User Sees |
|-----------------|-----------|
| `pitch_mean: 180, pitch_std: 45` | "Your voice has a medium pitch with rich melody that keeps listeners engaged" |
| `jitter_local: 0.8%, shimmer: 4.2%` | "Your voice quality is crystal clear with no shakiness" |
| `hnr_mean: 18 dB` | "Your voice sounds confident and resonant, not breathy" |
| `silence_ratio: 0.32, pause_count: 12` | "You used 12 well-timed pauses that make you sound thoughtful" |
| `rms_std: 0.08, dynamic_range: 15 dB` | "Your energy is dynamic with great peaks and valleys" |
| `filler_words: {um: 8, like: 12}` | "Try replacing those 20 fillers with brief pauses" |

---

## Data Flow

```
1. User uploads audio (base64)
   â†“
2. Backend decodes â†’ loads as waveform
   â†“
3. VAD segments speech/silence
   â†“
4. Extract acoustic features (pitch, loudness, quality, spectral)
   â†“
5. Whisper transcription
   â†“
6. Filler word detection
   â†“
7. Rule-based insights generator
   â†“
8. Enhanced GPT-4 prompt (transcript + acoustic context)
   â†“
9. Merge GPT + rule-based insights
   â†“
10. Save structured analysis to DB
   â†“
11. Frontend fetches & displays personalized insights
```

---

## Key Features

### âœ… Acoustic Analysis (Beyond Transcription)
- Pitch/F0 analysis (not GPT-guessed)
- RMS loudness & dynamic range
- Speech/silence timing
- Voice quality proxies

### âœ… Personalized Insights
- No technical jargon in UI
- Conversational language
- Specific examples from their speech
- Actionable tips

### âœ… Backward Compatible
- Legacy `analysis.archetype`, `analysis.strengths` still work
- Graceful fallbacks if new fields missing

### âœ… Flexible Architecture
- Rule-based layer (always works)
- GPT enhancement (when available)
- Modular feature extractors

---

## To Install & Run

### Backend
```bash
cd backend
pip install -r requirements.txt
# Ensure .env has OPENAI_API_KEY and MONGO_URL
python -m uvicorn server:app --reload
```

### Frontend
```bash
cd frontend
npm install
# Ensure .env has EXPO_PUBLIC_BACKEND_URL
npx expo start
```

---

## Testing

### Quick Test
1. Record a 30-60s voice note
2. Upload via app
3. Check results screen for:
   - Voice personality archetype
   - Conversational headline
   - Key insights (should be specific, not generic)
   - Strengths & growth opportunities in plain language
   - No Hz/dB/technical terms visible to user

### Verify Acoustic Analysis
Check MongoDB `assessments` collection:
- `analysis.technical.prosody.pitch_mean` should be >0
- `analysis.technical.timing.pause_count` should match actual pauses
- `analysis.insights.key_insights` should be conversational strings

---

## Future Enhancements

### Phase 2 (Optional)
- Replace librosa quality proxies with Parselmouth (real jitter/shimmer/HNR)
- Add formant analysis (F1, F2, F3)

### Phase 3 (Optional)
- Emotion recognition (SER model from SpeechBrain/HF)
- Word-level timestamps (faster-whisper)
- Align fillers + pauses on timeline

### Phase 4 (UI)
- Pitch/loudness timeline charts (react-native-svg)
- Pause visualization overlay
- Emotion timeline (if SER added)

---

## Architecture Decisions

### Why librosa over Parselmouth?
- **Easier cross-platform setup**: Parselmouth requires praat binaries
- **Proxies are good enough**: Spectral flatness/ZCR correlate well with quality
- Can upgrade later without changing architecture

### Why rule-based + GPT?
- **Reliability**: Rule-based always works, GPT enhances
- **Cost control**: Fewer tokens than asking GPT to analyze waveform
- **Accuracy**: Measured metrics > GPT guesses

### Why separate `insights` from `technical`?
- **UX clarity**: Frontend shows insights, hides technical
- **Debugging**: Devs can inspect raw metrics
- **Flexibility**: Easy to add "advanced view" later

---

## Summary

**Implemented**: Full acoustic voice analysis with personalized, user-friendly insights that avoid technical jargon.

**Key Innovation**: Translation layer that converts `pitch_std: 45Hz` â†’ "Your voice has natural melody."

**Architecture**: Modular, backward-compatible, reliable (rule-based + GPT enhancement).

**Next Steps**: Test with real recordings, gather user feedback on insight quality, consider Phase 2/3 enhancements.
